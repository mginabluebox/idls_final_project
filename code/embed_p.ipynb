{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing packages\n",
    "# # !pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# # !pip3 install jupyter jupyterhub pandas matplotlib scipy scikit-learn scikit-image Pillow\n",
    "# !pip3 install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "# !pip3 install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "# !pip3 install torch-cluster -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "# !pip3 install torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "# !pip3 install xgboost\n",
    "\n",
    "# importing libraries\n",
    "import os\n",
    "import json\n",
    "import csv \n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, CyclicLR\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xg\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#domain-index mapping\n",
    "with open(os.path.join('../data/reddit_index.json')) as f:\n",
    "    reddit_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subreddit-domain network\n",
    "df = pd.read_csv('../data/reddit_subreddit_to_domain__gt-01-urls.csv', header=None)\n",
    "source_nodes = df.iloc[:,0].apply(lambda x: reddit_dict[x]).values.tolist()\n",
    "target_nodes = df.iloc[:,1].apply(lambda x: reddit_dict[x]).values.tolist()\n",
    "num_nodes = len(set(source_nodes).union(set(target_nodes)))\n",
    "weight = df.iloc[:,2].values.tolist()\n",
    "edge_index = torch.tensor([source_nodes, target_nodes])\n",
    "edge_attr = torch.tensor(weight)[:,None]\n",
    "data = Data(edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "data.num_nodes = num_nodes\n",
    "transform = T.ToUndirected()\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target labels (political ideology score)\n",
    "domain_ideology = pd.read_csv('../data/robertson_et_al.csv')\n",
    "domain_ideology = domain_ideology[['domain', 'score']].copy()\n",
    "domain_ideology['id'] = domain_ideology['domain'].apply(lambda x: reddit_dict[x] if x in reddit_dict else None)\n",
    "domain_ideology = domain_ideology[domain_ideology['id'].notna()].reset_index(drop=True)\n",
    "domain_ideology['id'] = domain_ideology['id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test-split\n",
    "train = domain_ideology.sample(frac=0.8,random_state=42)\n",
    "test = domain_ideology[~domain_ideology.index.isin(train.index)]\n",
    "train_sub = train.sample(frac=0.8, random_state=24)\n",
    "val = train[~train.index.isin(train_sub.index)]\n",
    "\n",
    "train_x, train_y = train_sub['id'].tolist(), train_sub['score'].tolist()\n",
    "val_x, val_y = val['id'].tolist(), val['score'].tolist()\n",
    "test_x, test_y = test['id'].tolist(), test['score'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predict the political ideology of each node in the subreddit-domain network using:\n",
    "1. node2vec for creating(embedding) feature representations of the nodes of the graph\n",
    "2. ridge regression for predicting the political inclination of each node in the graph\n",
    "'''\n",
    "def predict(p=1, q=1, walk_length=10, classifier=\"Ridge\", start_epochs=0, num_epochs=30,\n",
    "            batch_size=128, learning_rate=0.01):\n",
    "    \n",
    "    model = Node2Vec(data.edge_index, embedding_dim=128, \n",
    "                 walk_length=walk_length, context_size=10,\n",
    "                 walks_per_node=10, num_negative_samples=1,\n",
    "                 p=p, q=q, sparse=True).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=learning_rate)\n",
    "    loader = model.loader(batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    \n",
    "    # scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, threshold=0.001, threshold_mode='rel', eps=1e-04)\n",
    "    \n",
    "    train_loss_list =[]\n",
    "    val_mse_list = []\n",
    "    \n",
    "    def train():\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    #     scheduler.step(loss)\n",
    "        return total_loss / len(loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(classifier=\"Ridge\"):\n",
    "        model.eval()\n",
    "        z = model()\n",
    "\n",
    "        if classifier==\"Ridge\":\n",
    "            clf = Ridge(alpha=0.01).fit(z[train_x].detach().cpu().numpy(), train_y)\n",
    "        elif classifier==\"RF\":\n",
    "            clf = RandomForestRegressor(max_depth=10, random_state=0).fit(z[train_x].detach().cpu().numpy(), train_y)  \n",
    "        elif classifier==\"XGB\":\n",
    "            clf = xg.XGBRegressor(objective ='reg:squarederror',\n",
    "                                  n_estimators = 10,\n",
    "                                  seed = 0).fit(z[train_x].detach().cpu().numpy(), train_y)\n",
    "\n",
    "        val_preds = clf.predict(z[val_x].detach().cpu().numpy())\n",
    "        train_preds = clf.predict(z[train_x].detach().cpu().numpy())\n",
    "        return mean_squared_error(train_y, train_preds), mean_squared_error(val_y, val_preds) \n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epochs+1, num_epochs+1):\n",
    "        loss = train()\n",
    "        train_mse, val_mse = test()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        train_loss_list.append(loss)\n",
    "        val_mse_list.append(val_mse)\n",
    "        \n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train MSE: {train_mse:.4f}, Val MSE: {val_mse:.4f}, Time: {(end_time-start_time)/60:.2f} mins')\n",
    "    return train_loss_list, val_mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizing train loss and validation MSE\n",
    "for a specific hyperparameter across multiple experiments\n",
    "'''\n",
    "def visualization(train_losses, val_mses, flag, tune_range):\n",
    "    if flag==\"p\":\n",
    "        title=\"Tuning p for Network Embedding\"\n",
    "    elif flag==\"q\":\n",
    "        title=\"Tuning q for Network Embedding\"\n",
    "    elif flag==\"walk\":\n",
    "        title=\"Tuning Random Walk Length for Network Embedding\"   \n",
    "    elif flag==\"batch_size\":\n",
    "        title=\"Tuning Batch-Size for Network Embedding\"   \n",
    "    elif flag==\"learning_rate\":\n",
    "        title=\"Tuning Learning Rate for Network Embedding\"           \n",
    "        \n",
    "    n_epochs = len(train_losses[0])\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    for i, param in enumerate(tune_range):\n",
    "        print(param)\n",
    "        plt.plot(train_losses[i], marker=\"o\", label=flag + \" = \" + str(param))\n",
    "    plt.xlabel(flag,size=14)\n",
    "    plt.ylabel(\"Train Loss\",size=14)\n",
    "    plt.title(title, size=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(flag+'_train_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    for i,param in enumerate(tune_range):\n",
    "        plt.plot(val_mses[i], marker=\"o\", label = flag + \" = \" + str(param))\n",
    "    plt.xlabel(flag,size=14)\n",
    "    plt.ylabel(\"Validation MSE\", size=14)\n",
    "    plt.title(title, size=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(flag+'_val_mse.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges for tuning hyperparameters for embedding algorithm\n",
    "p_range = [1, 2, 5, 10]\n",
    "q_range = [1, 2, 5, 10]\n",
    "walk_range = [10, 20, 50]\n",
    "batch_size_range = [32, 64, 128, 256]\n",
    "learning_rate_range = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "#selecting optimal p\n",
    "def p_tune(flag=\"p\", tune_range=p_range):\n",
    "    train_losses = []\n",
    "    val_mses = []\n",
    "    for param_value in tune_range:    \n",
    "        print(\"\\n\\n\", flag+\" : \", param_value, \"\\n\\n\")\n",
    "        train_loss_list, val_mse_list = predict(p=param_value)\n",
    "        train_losses.append(train_loss_list)\n",
    "        val_mses.append(val_mse_list)\n",
    "    np.save(flag+\"_val_mses.npy\", np.array(val_mses))\n",
    "    np.save(flag+\"_train_losses.npy\",np.array(train_losses))\n",
    "    val_mses = np.load(flag+\"_val_mses.npy\")\n",
    "    train_losses = np.load(flag+\"_train_losses.npy\")\n",
    "\n",
    "    visualization(train_losses, val_mses, flag=flag)\n",
    "\n",
    "#selecting optimal q\n",
    "def q_tune(flag=\"q\", tune_range=q_range):\n",
    "    train_losses = []\n",
    "    val_mses = []\n",
    "    for param_value in tune_range:    \n",
    "        print(\"\\n\\n\", flag+\" : \", param_value, \"\\n\\n\")\n",
    "        train_loss_list, val_mse_list = predict(p=param_value)\n",
    "        train_losses.append(train_loss_list)\n",
    "        val_mses.append(val_mse_list)\n",
    "    np.save(flag+\"_val_mses.npy\", np.array(val_mses))\n",
    "    np.save(flag+\"_train_losses.npy\",np.array(train_losses))\n",
    "    val_mses = np.load(flag+\"_val_mses.npy\")\n",
    "    train_losses = np.load(flag+\"_train_losses.npy\")\n",
    "\n",
    "    visualization(train_losses, val_mses, flag=flag)\n",
    "    \n",
    "#selecting optimal walk length\n",
    "def walk_tune(flag=\"walk\", tune_range=walk_range):\n",
    "    train_losses = []\n",
    "    val_mses = []\n",
    "    for param_value in tune_range:    \n",
    "        print(\"\\n\\n\", flag+\" : \", param_value, \"\\n\\n\")\n",
    "        train_loss_list, val_mse_list = predict(walk_length=param_value)\n",
    "        train_losses.append(train_loss_list)\n",
    "        val_mses.append(val_mse_list)\n",
    "    np.save(flag+\"_val_mses.npy\", np.array(val_mses))\n",
    "    np.save(flag+\"_train_losses.npy\",np.array(train_losses))\n",
    "    val_mses = np.load(flag+\"_val_mses.npy\")\n",
    "    train_losses = np.load(flag+\"_train_losses.npy\")\n",
    "\n",
    "    visualization(train_losses, val_mses, flag=flag)\n",
    "\n",
    "#selecting optimal batch size\n",
    "def batch_size_tune(flag=\"batch_size\", tune_range=batch_size_range):\n",
    "    train_losses = []\n",
    "    val_mses = []\n",
    "    for param_value in tune_range:    \n",
    "        print(\"\\n\\n\", flag+\" : \", param_value, \"\\n\\n\")\n",
    "        train_loss_list, val_mse_list = predict(batch_size=param_value, learning_rate = 0.01*(param_value/128))\n",
    "        train_losses.append(train_loss_list)\n",
    "        val_mses.append(val_mse_list)\n",
    "    np.save(flag+\"_val_mses.npy\", np.array(val_mses))\n",
    "    np.save(flag+\"_train_losses.npy\",np.array(train_losses))\n",
    "    val_mses = np.load(flag+\"_val_mses.npy\")\n",
    "    train_losses = np.load(flag+\"_train_losses.npy\")\n",
    "\n",
    "    visualization(train_losses, val_mses, flag=flag, tune_range=tune_range)\n",
    "\n",
    "#selecting optimal learning rate\n",
    "def learning_rate_tune(flag=\"learning_rate\", tune_range=learning_rate_range):\n",
    "    train_losses = []\n",
    "    val_mses = []\n",
    "    for param_value in tune_range:    \n",
    "        print(\"\\n\\n\", flag+\" : \", param_value, \"\\n\\n\")\n",
    "        train_loss_list, val_mse_list = predict(learning_rate=param_value)\n",
    "        train_losses.append(train_loss_list)\n",
    "        val_mses.append(val_mse_list)\n",
    "    np.save(flag+\"_val_mses.npy\", np.array(val_mses))\n",
    "    np.save(flag+\"_train_losses.npy\",np.array(train_losses))\n",
    "    val_mses = np.load(flag+\"_val_mses.npy\")\n",
    "    train_losses = np.load(flag+\"_train_losses.npy\")\n",
    "\n",
    "    visualization(train_losses, val_mses, flag=flag, tune_range=tune_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
